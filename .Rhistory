white_wine <- read.csv("Data/wine+quality/winequality-white.csv", header = TRUE, sep = ";")
head(white_wine)
str(white_wine)
round(cor(white_wine),2)
# Load necessary libraries
library(caret)
library(e1071)
library(randomForest)
library(tidyverse)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
# splitting data into dependent and independent variables
X <- white_wine[, -12]  # Exclude the 'quality' column
y <- white_wine$quality
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[trainIndex, ]
y_train <- y[trainIndex]
X_test <- X[-trainIndex, ]
y_test <- y[-trainIndex]
# Function to train and evaluate a model
train_and_evaluate <- function(model, X_train, y_train, X_test, y_test) {
fit <- train(y_train ~ ., data = data.frame(X_train, y_train), method = model)
predictions <- predict(fit, newdata = data.frame(X_test))
rmse <- RMSE(predictions, y_test)
return(rmse)
}
# Ridge and Lasso
model_types <- list(
lm = "lm",
ridge = "ridge",
lasso = "lasso",
svm = "svmRadial"  # Support Vector Regression with radial basis function
)
# Evaluate each model
results <- list()
for (name in names(model_types)) {
model <- model_types[[name]]
rmse <- train_and_evaluate(model, X_train, y_train, X_test, y_test)
results[[name]] <- rmse
}
stopCluster(cl)
# Display results
for (name in names(results)) {
cat(name, "RMSE:", results[[name]], "\n")
}
white_wine <- read.csv("Data/wine+quality/winequality-white.csv", header = TRUE, sep = ";")
head(white_wine)
library(e1071)
library(caret)
library(pls)
# Assuming white_wine data is already loaded
# Normalize data and perform PCA
pca_result <- prcomp(white_wine[, sapply(white_wine, is.numeric)], scale. = TRUE)
# Find the number of components to explain 80% of variance
cum_var <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
num_comps <- which(cum_var >= 0.8)[1]
# Prepare the PCA-reduced dataset
pca_data <- data.frame(pca_result$x[, 1:num_comps])
pca_data$quality <- white_wine$quality  # Add the target variable back
# Split data into training and testing sets
set.seed(123)  # for reproducibility
train_index <- createDataPartition(white_wine$quality, p = 0.8, list = FALSE)
train_data <- white_wine[train_index, ]
test_data <- white_wine[-train_index, ]
train_data_pca <- pca_data[train_index, ]
test_data_pca <- pca_data[-train_index, ]
# Set up training control
train_control <- trainControl(method = "cv", number = 5, savePredictions = "final")
# Define tuning grid
tuning_grid <- expand.grid(.C=10, .sigma=1/(2*1))  # Adjust sigma for gamma = 1
# SVM on original training data
svm_original <- train(quality ~ ., data = train_data, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
library(e1071)
library(caret)
library(pls)
# Assuming white_wine data is already loaded
# Normalize data and perform PCA
pca_result <- prcomp(white_wine[, sapply(white_wine, is.numeric)], scale. = TRUE)
# Find the number of components to explain 80% of variance
cum_var <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
num_comps <- which(cum_var >= 0.8)[1]
# Prepare the PCA-reduced dataset
pca_data <- data.frame(pca_result$x[, 1:num_comps])
pca_data$quality <- white_wine$quality  # Add the target variable back
# Split data into training and testing sets
set.seed(123)  # for reproducibility
train_index <- createDataPartition(white_wine$quality, p = 0.8, list = FALSE)
train_data <- white_wine[train_index, ]
test_data <- white_wine[-train_index, ]
train_data_pca <- pca_data[train_index, ]
test_data_pca <- pca_data[-train_index, ]
# Set up training control
train_control <- trainControl(method = "cv", number = 5, savePredictions = "final")
# Define tuning grid
tuning_grid <- expand.grid(.C=10, .sigma=1)  # Adjust sigma for gamma = 1
# SVM on original training data
svm_original <- train(quality ~ ., data = train_data, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
# SVM on PCA-reduced training data
svm_pca <- train(quality ~ ., data = train_data_pca, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
# Predict and evaluate on the test set
predictions_original <- predict(svm_original, newdata = test_data)
predictions_pca <- predict(svm_pca, newdata = test_data_pca)
# Calculate RMSE for both models
rmse_original <- sqrt(mean((predictions_original - test_data$quality)^2))
cat("RMSE for original data on test set:", rmse_original, "\n")
rmse_pca <- sqrt(mean((predictions_pca - test_data_pca$quality)^2))
cat("RMSE for PCA-reduced data on test set:", rmse_pca, "\n")
library(e1071)
library(caret)
library(pls)
# Assuming white_wine data is already loaded
# Normalize data and perform PCA
pca_result <- prcomp(white_wine[, sapply(white_wine, is.numeric)], scale. = TRUE)
# Find the number of components to explain 80% of variance
cum_var <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
num_comps <- which(cum_var >= 0.8)[1]
# Prepare the PCA-reduced dataset
pca_data <- data.frame(pca_result$x[, 1:num_comps])
pca_data$quality <- white_wine$quality  # Add the target variable back
# Split data into training and testing sets
set.seed(123)  # for reproducibility
train_index <- createDataPartition(white_wine$quality, p = 0.8, list = FALSE)
train_data <- white_wine[train_index, ]
test_data <- white_wine[-train_index, ]
train_data_pca <- pca_data[train_index, ]
test_data_pca <- pca_data[-train_index, ]
# Set up training control
train_control <- trainControl(method = "cv", number = 5, savePredictions = "final")
# Define tuning grid
tuning_grid <- expand.grid(.C=10, .sigma=1)  # Adjust sigma for gamma = 1
# SVM on original training data
svm_original <- train(quality ~ ., data = train_data, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
# SVM on PCA-reduced training data
svm_pca <- train(quality ~ ., data = train_data_pca, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
# Predict and evaluate on the test set
predictions_original <- predict(svm_original, newdata = test_data)
predictions_pca <- predict(svm_pca, newdata = test_data_pca)
# Calculate RMSE for both models
rmse_original <- sqrt(mean((predictions_original - test_data$quality)^2))
cat("RMSE for original data on test set:", rmse_original, "\n")
rmse_pca <- sqrt(mean((predictions_pca - test_data_pca$quality)^2))
cat("RMSE for PCA-reduced data on test set:", rmse_pca, "\n")
library(e1071)
library(caret)
library(pls)
library(e1071)
library(caret)
library(pls)
# Assuming 'white_wine' is your dataset
# Exclude the 'quality' target variable before PCA
features <- white_wine[, sapply(white_wine, is.numeric) & names(white_wine) != "quality"]
pca_result <- prcomp(features, scale. = TRUE)
# Determine the number of components to retain
cum_var <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
num_comps <- which(cum_var >= 0.8)[1]
# Create a PCA-reduced dataset
pca_data <- data.frame(pca_result$x[, 1:num_comps])
# Add the 'quality' column back for training the SVM
pca_data$quality <- white_wine$quality
# Continue with the data splitting, SVM training, etc.
# Split data into training and testing sets
set.seed(123)  # for reproducibility
train_index <- createDataPartition(white_wine$quality, p = 0.8, list = FALSE)
train_data <- white_wine[train_index, ]
test_data <- white_wine[-train_index, ]
train_data_pca <- pca_data[train_index, ]
test_data_pca <- pca_data[-train_index, ]
# Set up training control
train_control <- trainControl(method = "cv", number = 5, savePredictions = "final")
# Define tuning grid
tuning_grid <- expand.grid(.C=10, .sigma=1)  # Adjust sigma for gamma = 1
# SVM on original training data
svm_original <- train(quality ~ ., data = train_data, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
# SVM on PCA-reduced training data
svm_pca <- train(quality ~ ., data = train_data_pca, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
# Predict and evaluate on the test set
predictions_original <- predict(svm_original, newdata = test_data)
predictions_pca <- predict(svm_pca, newdata = test_data_pca)
# Calculate RMSE for both models
rmse_original <- sqrt(mean((predictions_original - test_data$quality)^2))
cat("RMSE for original data on test set:", rmse_original, "\n")
rmse_pca <- sqrt(mean((predictions_pca - test_data_pca$quality)^2))
cat("RMSE for PCA-reduced data on test set:", rmse_pca, "\n")
library(e1071)
library(caret)
library(pls)
library(e1071)
library(caret)
library(pls)
# Assuming 'white_wine' is your dataset
# Exclude the 'quality' target variable before PCA
features <- white_wine[, sapply(white_wine, is.numeric) & names(white_wine) != "quality"]
pca_result <- prcomp(features, scale. = TRUE)
# Determine the number of components to retain
cum_var <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
num_comps <- which(cum_var >= 0.8)[1]
# Create a PCA-reduced dataset
pca_data <- data.frame(pca_result$x[, 1:num_comps])
# Add the 'quality' column back for training the SVM
pca_data$quality <- white_wine$quality
# Continue with the data splitting, SVM training, etc.
# Split data into training and testing sets
set.seed(123)  # for reproducibility
train_index <- createDataPartition(white_wine$quality, p = 0.8, list = FALSE)
train_data <- white_wine[train_index, ]
test_data <- white_wine[-train_index, ]
train_data_pca <- pca_data[train_index, ]
test_data_pca <- pca_data[-train_index, ]
# Set up training control
train_control <- trainControl(method = "cv", number = 5, savePredictions = "final")
# Define tuning grid
tuning_grid <- expand.grid(.C=10, .sigma=1)  # Adjust sigma for gamma = 1
# SVM on original training data
svm_original <- train(quality ~ ., data = train_data, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
library(e1071)
library(caret)
library(pls)
library(e1071)
library(caret)
library(pls)
# Assuming 'white_wine' is your dataset
# Exclude the 'quality' target variable before PCA
features <- white_wine[, sapply(white_wine, is.numeric) & names(white_wine) != "quality"]
pca_result <- prcomp(features, scale. = TRUE)
# Determine the number of components to retain
cum_var <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
num_comps <- which(cum_var >= 0.8)[1]
num_comps
# Create a PCA-reduced dataset
pca_data <- data.frame(pca_result$x[, 1:num_comps])
# Add the 'quality' column back for training the SVM
pca_data$quality <- white_wine$quality
# Continue with the data splitting, SVM training, etc.
# Split data into training and testing sets
set.seed(123)  # for reproducibility
train_index <- createDataPartition(white_wine$quality, p = 0.8, list = FALSE)
train_data <- white_wine[train_index, ]
test_data <- white_wine[-train_index, ]
train_data_pca <- pca_data[train_index, ]
test_data_pca <- pca_data[-train_index, ]
# Set up training control
train_control <- trainControl(method = "cv", number = 5, savePredictions = "final")
# Define tuning grid
tuning_grid <- expand.grid(.C=10, .sigma=1)  # Adjust sigma for gamma = 1
# SVM on original training data
svm_original <- train(quality ~ ., data = train_data, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
# SVM on PCA-reduced training data
svm_pca <- train(quality ~ ., data = train_data_pca, method = "svmRadial",
trControl = train_control, metric = "RMSE", preProcess = c("center", "scale"),
tuneGrid = tuning_grid)
# Predict and evaluate on the test set
predictions_original <- predict(svm_original, newdata = test_data)
predictions_pca <- predict(svm_pca, newdata = test_data_pca)
# Calculate RMSE for both models
rmse_original <- sqrt(mean((predictions_original - test_data$quality)^2))
cat("RMSE for original data on test set:", rmse_original, "\n")
rmse_pca <- sqrt(mean((predictions_pca - test_data_pca$quality)^2))
cat("RMSE for PCA-reduced data on test set:", rmse_pca, "\n")
brary(e1071)
#required packages installation.. please uncomment any of the following packages if you don't have them already installed
#install.packages("caret")
#install.packages("corrplot")
#install.packages("tidyverse")
#install.packages("readr")
#install.packages("dplyr")
#install.packages("DataExplorer")
#install.packages("GGally")
white_wine <- read.csv("Data/wine+quality/winequality-white.csv", header = TRUE, sep = ";")
head(white_wine)
dim(white_wine)
str(white_wine)
sum(is.na(white_wine)) # checking if there is null values
sum(duplicated(white_wine)) # checking if there are duplicates
stats_summary <- summary(white_wine)
print(stats_summary)
correlation <- cor(white_wine)
correlation <- round(correlation, 2)
correlation
#visualizing for better readablity
library(corrplot)
corrplot(correlation, type = "lower", order = "hclust",
tl.col = "black", tl.srt = 45,
title= "Correlation of Wine Quality Attributes",
mar = c(0,0,2,0) # adjusting margins to show the title (bottom, left, top, right)
)
# count of each rating of the quality column
quality_counts <- table(white_wine$quality)
quality_counts
# percentage of each rating
quality_percentage <- quality_counts / nrow(white_wine) * 100
round(quality_percentage, 2)
library(caret) # required library
nearZeroVar(white_wine, saveMetrics = TRUE)
library(ggplot2) # required library for plotting
for(i in names(white_wine)[-12]) { #
p <- ggplot(white_wine, aes_string(x = i)) +
geom_histogram(bins = 30, fill = "lightblue", color = "black") +
ggtitle(paste("Histogram of", i)) +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5))
print(p)
}
for(i in names(white_wine)[-12]) {
p <- ggplot(white_wine, aes_string(x = i)) +
geom_density(fill = "lightblue", color = "black", alpha = 0.5) +  # Density plot
ggtitle(paste("Density and Skewness of", i)) +
theme_minimal()
print(p)
}
getwd()
predict_quality <- function(data_path, preproc_path, model_path) {
# 1- Load the dataset
new_data <- read.csv(data_path, header = TRUE, sep = ";")
# 2- remove quality column if it exists
if("quality" %in% colnames(new_data)){
new_data$quality <- NULL
}
# 3- Apply preprocessig (scaling)
preProcessingValues <- readRDS(preproc_path)
new_data_scaled <- predict(preProcessingValues, new_data)
new_data_scaled <- as.data.frame(new_data_scaled)
# 4- predict the quality using the saved model
svm_model <- readRDS(model_path)
predictions <- predict(svm_model, newdata = new_data_scaled)
integer_predictions <- round(predictions)
# 5- Add predictions as a new 'quality' column
new_data_scaled$quality <- integer_predictions
# Return the new dataset with predictions
return(new_data_scaled)
}
# Example usage:
# put ur own data path for prediction
data_path <- "Data/wine+quality/winequality-white.csv"
preproc_path <- "preProcessingValues.rds"
model_path <- "svm_model.rds"
predicted_data <- predict_quality(data_path, preproc_path, model_path)
predicted_data
source("M:/Wine_Quality_Analysis/deployment.R", echo=TRUE)
predict_quality <- function(data_path, preproc_path, model_path) {
# 1- Load the dataset
new_data <- read.csv(data_path, header = TRUE, sep = ";")
# 2- remove quality column if it exists
if("quality" %in% colnames(new_data)){
new_data$quality <- NULL
}
# 3- Apply preprocessig (scaling)
preProcessingValues <- readRDS(preproc_path)
new_data_scaled <- predict(preProcessingValues, new_data)
new_data_scaled <- as.data.frame(new_data_scaled)
# 4- predict the quality using the saved model
svm_model <- readRDS(model_path)
predictions <- predict(svm_model, newdata = new_data_scaled)
integer_predictions <- round(predictions,2)
# 5- Add predictions as a new 'quality' column
new_data_scaled$quality <- integer_predictions
# Return the new dataset with predictions
return(new_data_scaled)
}
# Example usage:
# put ur own data path for prediction
data_path <- "Data/wine+quality/winequality-white.csv"
preproc_path <- "preProcessingValues.rds"
model_path <- "svm_model.rds"
predicted_data <- predict_quality(data_path, preproc_path, model_path)
predicted_data
source("M:/Wine_Quality_Analysis/deployment.R", echo=TRUE)
source("M:/Wine_Quality_Analysis/deployment.R", echo=TRUE)
source("M:/Wine_Quality_Analysis/deployment.R", echo=TRUE)
View(predicted_data)
View(predicted_data)
predict_quality <- function(data_path, preproc_path, model_path) {
# 1- Load the dataset
new_data <- read.csv(data_path, header = TRUE, sep = ";")
# 2- remove quality column if it exists
if("quality" %in% colnames(new_data)){
new_data$quality <- NULL
}
# 3- Apply preprocessig (scaling)
preProcessingValues <- readRDS(preproc_path)
new_data_scaled <- predict(preProcessingValues, new_data)
new_data_scaled <- as.data.frame(new_data_scaled)
# 4- predict the quality using the saved model
svm_model <- readRDS(model_path)
predictions <- predict(svm_model, newdata = new_data_scaled)
integer_predictions <- round(predictions)
# 5- Add predictions as a new 'quality' column
new_data_scaled$quality <- integer_predictions
# Return the new dataset with predictions
return(new_data_scaled)
}
# Example usage:
# put ur own data path for prediction
data_path <- "Data/wine+quality/winequality-white.csv"
preproc_path <- "preProcessingValues.rds"
model_path <- "svm_model.rds"
predicted_data <- predict_quality(data_path, preproc_path, model_path)
predicted_data
table(predicted_data$quality)
predict_quality <- function(data_path, preproc_path, model_path) {
# 1- Load the dataset
new_data <- read.csv(data_path, header = TRUE, sep = ";")
# 2- remove quality column if it exists
if("quality" %in% colnames(new_data)){
new_data$quality <- NULL
}
# 3- Apply preprocessig (scaling)
preProcessingValues <- readRDS(preproc_path)
new_data_scaled <- predict(preProcessingValues, new_data)
new_data_scaled <- as.data.frame(new_data_scaled)
# 4- predict the quality using the saved model
svm_model <- readRDS(model_path)
predictions <- predict(svm_model, newdata = new_data_scaled)
integer_predictions <- round(predictions)
# 5- Add predictions as a new 'quality' column
new_data_scaled$quality <- integer_predictions
# Return the new dataset with predictions
return(new_data_scaled)
}
# Example usage:
# put ur own data path for prediction
data_path <- "Data/wine+quality/winequality-white.csv"
preproc_path <- "preProcessingValues.rds"
model_path <- "svm_model.rds"
predicted_data <- predict_quality(data_path, preproc_path, model_path)
predicted_data
View(white_wine)
white_wine <- read.csv("Data/wine+quality/winequality-white.csv", header = TRUE, sep = ";")
head(white_wine)
preProcessingValues <- preProcess(white_wine[,-12], method = c("center", "scale"))
library(caret)
library(e1071)
library(randomForest)
library(tidyverse)
library(doParallel)
library(dplyr)
preProcessingValues <- preProcess(white_wine[,-12], method = c("center", "scale"))
white_wine_scaled <- predict(preProcessingValues, white_wine[, -12])
saveRDS(preProcessingValues, file = "preProcessingValues.rds")
saveRDS(preProcessingValues, file = "preprocessing/preProcessingValues.rds")
predict_quality <- function(data_path, preproc_path, model_path) {
# 1- Load the dataset
new_data <- read.csv(data_path, header = TRUE, sep = ";")
# 2- remove quality column if it exists
if("quality" %in% colnames(new_data)){
new_data$quality <- NULL
}
# 3- Apply preprocessig (scaling)
preProcessingValues <- readRDS(preproc_path)
new_data_scaled <- predict(preProcessingValues, new_data)
new_data_scaled <- as.data.frame(new_data_scaled)
# 4- predict the quality using the saved model
svm_model <- readRDS(model_path)
predictions <- predict(svm_model, newdata = new_data_scaled)
integer_predictions <- round(predictions)
# 5- Add predictions as a new 'quality' column
new_data_scaled$quality <- integer_predictions
# Return the new dataset with predictions
return(new_data_scaled)
}
# Example usage:
# put ur own data path for prediction
data_path <- "Data/wine+quality/winequality-white.csv"
preproc_path <- "preprocessing/preProcessingValues.rds"
model_path <- "models/svm_model.rds"
predicted_data <- predict_quality(data_path, preproc_path, model_path)
predicted_data
